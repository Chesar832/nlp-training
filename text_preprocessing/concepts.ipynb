{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description:\n",
    "Text preprocessing is the first step in NLP. It involves cleaning and transforming raw text data into a format that can be easily analyzed by machine learning algorithms. Common tasks include tokenization, lemmatization, and punctuation removal.\n",
    "\n",
    "### Internal Topics:\n",
    "- Tokenization\n",
    "- Lemmatization\n",
    "- Punctuation Removal\n",
    "- Handling Multilingual Text\n",
    "- Dealing with Text with Spelling Errors\n",
    "\n",
    "### Exercise:\n",
    "Preprocess a dataset of movie reviews by removing stop words and applying lemmatization using SpaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a kind of segmentation for docs or sentences which consists of **breaking up text in smaller chunks**.   \n",
    "For that reason the text loses information when docs and tokens are compared in terms of information level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*PZYP2nL6Zc_jpkaHLRxLQQ.png\" alt=\"example\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the previous image, the text is splitted in four parts and one of them is a puncuation mark (!). This is a specific type of tokenization where the punctuation marks are considered as tokens but this kind of behaviours are chosen accord to the problem that is trying to resolved with NLP.  \n",
    "Indeed there's different kinds of tokenizations as the following:\n",
    "1. [Word tokenization](#1.-Word-tokenization)\n",
    "2. Subword tokenization\n",
    "3. Morphological tokenization\n",
    "4. Character tokenization\n",
    "5. Emojis-considered tokenization\n",
    "6. Byte based tokenization\n",
    "7. Sentences pieces (*N-grams*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best *tokenizer* is not always the same one, so the selection of the best option is a critical task because it will influence greatfully in the next steps in a *NLP pipeline*.  \n",
    "In the next image it can be observed that it's a NLP system / pipeline for a simple classification task based on a *corpus* composed by tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://almablog-media.s3.ap-south-1.amazonaws.com/NLP_Pipeline_deca032413.png\" alt=\"example\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for this section is from Kaggle and is available [here](https://www.kaggle.com/datasets/gpreda/pfizer-vaccine-tweets/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic tokenizer consists of split text in words based just in the blank spaces. This approach could consider punctuation marks as separated tokens, but they could also be ignored or be asociated by specific words, anyways this depends of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RANDOM_STATE = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/vaccination_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = df.sample(1, random_state=RANDOM_STATE)['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is driving protests in 'calm,consensus driven' #Netherlands? Resident @joezl looks at confluence of multiple e… https://t.co/hcjJrD93uO\n"
     ]
    }
   ],
   "source": [
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "for word in str(chain).split(sep=\" \"):\n",
    "    tokens.append(word)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'driving', 'protests', 'in', \"'calm,consensus\", \"driven'\", '#Netherlands?', 'Resident', '@joezl', 'looks', 'at', 'confluence', 'of', 'multiple', 'e…', 'https://t.co/hcjJrD93uO']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[What, is, driving, protests, in, ', calm, ,, consensus, driven, ', #, Netherlands, ?, Resident, @joezl, looks, at, confluence, of, multiple, e, …, https://t.co/hcjJrD93uO]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(chain)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    tokens.append(token)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main differences are the type of the tokens (`str` vs `spacy Token`) and the kind of interpretation.  \n",
    "`Spacy`, even when it is also using its blank tokenization, still separates punctation marks as commas and other more sophisticated  marks apostrophes. See the tokens for `\"'calm,consensus\"` and `\"driven'\"` for both approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
